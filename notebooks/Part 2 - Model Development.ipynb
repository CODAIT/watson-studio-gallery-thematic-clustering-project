{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Part 2 - Model Development\n# Modeling Using IBM Debater\u00ae Thematic Clustering of Sentences\n\n### Table of Contents\n\n* [0. Prerequisite](#prerequisite)\n* [1. Load Data](#1)\n* [2. Modeling](#2)   \n* [3. Testing](#3)\n* [4. Example](#4)\n* [Authors](#authors)\n\n<a class=\"anchor\" id=\"prerequisite\"></a>\n### 0. Prerequisites\n\nBefore you run this notebook complete the following steps:\n- Insert a project token\n- Import required modules\n\n#### Insert a project token\n\nWhen you import this project from the Watson Studio Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n\n```python\n# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\npc = project.project_context\n```\n\nIf you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n\n* Click on `More -> Insert project token` in the top-right menu section\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n* This should insert a cell at the top of this notebook similar to the example given above.\n\n  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n\n* Run the newly inserted cell before proceeding with the notebook execution below\n\n#### Import required modules\n\nImport and configure the required modules.\n"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nfrom ast import literal_eval\nfrom collections import defaultdict\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Load Data <a class=\"anchor\" id=\"1\"></a>\n\nRecall that in the first notebook (Part 1 - Data Exploration & Visualization), we modified the original dataset and saved two files to our Watson Studio project. We now need to load this files.\n\n\nThis function below will help load the files saved in our Watson Studio project."}, {"metadata": {}, "cell_type": "code", "source": "# Define get data file function\ndef get_file_handle(fname):\n    # Project data path for the raw data file\n    data_path = project.get_file(fname)\n    data_path.seek(0)\n    return data_path", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we can get the datasets."}, {"metadata": {}, "cell_type": "code", "source": "# Define filenames\nDATA_PATHS = ['themes.csv', 'groups_of_themes.csv']\n\n# Use pandas to read the data \ndf, groups_of_themes = [pd.read_csv(get_file_handle(data_path)) for data_path in DATA_PATHS]", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check loaded dataframe\ndf.head()", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "         Article Title                                           Sentence  \\\n0  Moeller High School  Moeller's student-run newspaper, The Crusader,...   \n1  Moeller High School  In 2008, The Crusader won First Place, the sec...   \n2  Moeller High School  The Squire is a student literary journal that ...   \n3  Moeller High School  Paul Keels - play-by-play announcer for Ohio S...   \n4  Moeller High School           Joe Uecker - Ohio State Senator (R-66) .   \n\n            SectionTitle                                       Article Link  \\\n0   School publications   https://en.wikipedia.org/wiki/Moeller_High_School   \n1   School publications   https://en.wikipedia.org/wiki/Moeller_High_School   \n2   School publications   https://en.wikipedia.org/wiki/Moeller_High_School   \n3        Notable alumni   https://en.wikipedia.org/wiki/Moeller_High_School   \n4        Notable alumni   https://en.wikipedia.org/wiki/Moeller_High_School   \n\n                                     label  label_id  \n0  Moeller_High_School:School_publications      3414  \n1  Moeller_High_School:School_publications      3414  \n2  Moeller_High_School:School_publications      3414  \n3       Moeller_High_School:Notable_alumni      3413  \n4       Moeller_High_School:Notable_alumni      3413  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article Title</th>\n      <th>Sentence</th>\n      <th>SectionTitle</th>\n      <th>Article Link</th>\n      <th>label</th>\n      <th>label_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Moeller High School</td>\n      <td>Moeller's student-run newspaper, The Crusader,...</td>\n      <td>School publications</td>\n      <td>https://en.wikipedia.org/wiki/Moeller_High_School</td>\n      <td>Moeller_High_School:School_publications</td>\n      <td>3414</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Moeller High School</td>\n      <td>In 2008, The Crusader won First Place, the sec...</td>\n      <td>School publications</td>\n      <td>https://en.wikipedia.org/wiki/Moeller_High_School</td>\n      <td>Moeller_High_School:School_publications</td>\n      <td>3414</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Moeller High School</td>\n      <td>The Squire is a student literary journal that ...</td>\n      <td>School publications</td>\n      <td>https://en.wikipedia.org/wiki/Moeller_High_School</td>\n      <td>Moeller_High_School:School_publications</td>\n      <td>3414</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Moeller High School</td>\n      <td>Paul Keels - play-by-play announcer for Ohio S...</td>\n      <td>Notable alumni</td>\n      <td>https://en.wikipedia.org/wiki/Moeller_High_School</td>\n      <td>Moeller_High_School:Notable_alumni</td>\n      <td>3413</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Moeller High School</td>\n      <td>Joe Uecker - Ohio State Senator (R-66) .</td>\n      <td>Notable alumni</td>\n      <td>https://en.wikipedia.org/wiki/Moeller_High_School</td>\n      <td>Moeller_High_School:Notable_alumni</td>\n      <td>3413</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Check loaded dataframe\ngroups_of_themes.head()", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "                            group\n0  [2822, 1492, 2014, 4508, 4393]\n1   [535, 2896, 3550, 1670, 2837]\n2    [739, 659, 1015, 1362, 3938]\n3  [4167, 4753, 1516, 1386, 1705]\n4  [3029, 3826, 3057, 3969, 5299]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2822, 1492, 2014, 4508, 4393]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[535, 2896, 3550, 1670, 2837]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[739, 659, 1015, 1362, 3938]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[4167, 4753, 1516, 1386, 1705]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[3029, 3826, 3057, 3969, 5299]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "We convert `groups_of_themes` to a list of groups (`list_of_groups`) to more easily used in section 2."}, {"metadata": {}, "cell_type": "code", "source": "# Convert groups_of_themes to list of lists\ngroups_of_themes['group'] = groups_of_themes['group'].apply(lambda x: literal_eval(x))\nlist_of_groups = groups_of_themes.group.values.tolist()", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we are ready to create a model."}, {"metadata": {}, "cell_type": "markdown", "source": "###  2. Modeling <a class=\"anchor\" id=\"2\"></a>\n\nIn this section, we will create the clustering model. The model we create will be evaluated using the processed data we just loaded in section 1.\n\nTo understand the model it may be important to understand these definitions:\n* __TF-IDF__ (term frequency inverse document frequency) is a statistic that informs you how important a word is to a document in a corpus)\n* __document__ is an input text. So if we wanted to cluster a list of sentences, each sentence would be a document.\n* __corpus__ is a collection of text, so in the above example it would be the list of sentences.\n* __stopwords__ are common words in the English language that are removed to ensure that they are not the most influential words in an NLP model. For example, you would not the word \"the\" to be the most important feature.\n* __unigram__ is a single term (e.g. \"how\"), __bigram__ is a string of two terms (e.g. \"how are\"), __trigram__ is a string of three terms (e.g. \"how are you\")\n\nNow to create the clustering model, we use the following steps:\n1. Create a TF-IDF matrix using the input text. You will notice that in [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) there are parameters:\n    * max_df=0.75 means that all terms in more than 75% of documents (the documents means the entire text input) will be ignored\n    * min_df=0.1 means that all terms in less than 10% of documents will be ignored\n    * stop_words='english' means Sklearn's stop words are removed from the input text\n    * ngram_range = (1,3) means that unigrams, bigrams, and trigrams are used\n2. Then this matrix is used in a KMeans model.\n    * Number of clusters is the input.\n    * If no number of clusters is input then we determine the best number of clusters using [silhouette scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n3. Additionally, a function is written to extract the top terms used to cluster the text.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "The following function `get_top_n_terms_per_cluster()` does step 3 and will be called in `run_model()`.\nThe purpose of this function is to get the top terms that was used to cluster the text so that someone can view and better understand and patterns."}, {"metadata": {}, "cell_type": "code", "source": "def get_top_n_terms_per_cluster(km_model, terms, n=5):\n    \"\"\"\n    Gets the top terms used to cluster text\n    \n    :param km_model: KMeans model\n    :param terms: list of terms from a TfidfVecotrizer object\n    :return: dictionary mapping cluster number to top n terms\n             {cluster_number: [term1, term2,..., termn]}\n    \"\"\"\n    cluster_terms = defaultdict(list)\n    \n    order_centroids = km_model.cluster_centers_.argsort()[:, ::-1]\n    for i in range(len(order_centroids)):\n        cluster = order_centroids[i]\n        for term_idx in cluster[:n]:\n            cluster_terms[i].append(terms[term_idx])\n            \n    return cluster_terms", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next function is `run_kmeans()` which uses Python library `sklearn` to create a [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) model (a type of clustering model). KMeans essentially uses the distance between points (each comment or text) to find the best clusters. The distances are calucated using a TF-IDF matrix. This TF-IDF matrix is created in `run_model()`."}, {"metadata": {}, "cell_type": "code", "source": "def run_kmeans(number_of_clusters, tfidf_matrix):\n    \"\"\"\n    :param number_of_clusters: int\n    :param tfidf_matrix: matrix from TfidfVectorizer object\n    :return: KMeans model, list of cluster labels\n    \"\"\"\n    km_model = KMeans(n_clusters=number_of_clusters, init='k-means++')\n    km_model.fit(tfidf_matrix.toarray())\n    clusters = km_model.labels_.tolist()\n    return km_model, clusters", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, `run_model()` puts together the entire process by 1) creating TF-IDF matrix from the input text, 2) running KMeans model, and 3) get the top terms used to create the clusters.\n\nThis method returns\n1. `best_clusters` which is a list of the cluster labels for each text\n    * e.g. [0, 1, 1, 2, 0] means that there were 5 input texts and the model created 3 clusters\n    \n    \n2. `cluster_terms` which is a dictionary mapping the cluster label to the top terms used to create that cluster.\n    * e.g. {0: ['money', 'price'], 1: ['customer', 'service'], 2: ['online', 'web']}"}, {"metadata": {}, "cell_type": "code", "source": "def run_model(X, number_of_clusters=None, number_of_terms=5, max_number_of_groups=5):\n    \"\"\"\n    Runs the entire modeling process\n    1. create TFIDF matrix\n    2. run KMeans with TFIDF matrix\n    3. get top terms used\n    \n    :return: (list of cluster assignments, \n              dictionary mapping cluster number of terms)\n    \"\"\"\n    # First find TFIDF matrix\n    tfidf_vectorizer = TfidfVectorizer(max_df=0.75 if len(X)>1 else 1, \n                                       min_df=0.1 if len(X)>1 else 1,\n                                       stop_words='english',\n                                       use_idf=True, \n                                       ngram_range=(1,3),\n                                      )\n\n    tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n    terms = tfidf_vectorizer.get_feature_names()\n    \n    # Number of clusters must be > 2 for silhouette_score to work.\n    # If there are 2 or less comments, then just set to number of comments.\n    number_of_clusters = len(X) if len(X) <= 2 else number_of_clusters\n    \n    if number_of_clusters:\n        # If there's a specific number of clusters specified, then run with that number.\n        km_model, best_clusters = run_kmeans(number_of_clusters, tfidf_matrix)\n        cluster_terms = get_top_n_terms_per_cluster(km_model, terms, number_of_terms)\n    else:\n        # Automatically find number of clusters with silhouette_score\n        # but have a maximum of max_number_of_groups \n        max_silhouette_score = 0\n        for k in range(2, min(max_number_of_groups, len(X))):\n            km_model, clusters = run_kmeans(k, tfidf_matrix)\n            current_silhouette_score = silhouette_score(tfidf_matrix, clusters)\n            if current_silhouette_score > max_silhouette_score:\n                max_silhouette_score = current_silhouette_score\n                cluster_terms = get_top_n_terms_per_cluster(km_model, terms, number_of_terms)\n                best_clusters = clusters\n            \n    return best_clusters, cluster_terms", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3 Testing Model  <a class=\"anchor\" id=\"3\"></a>\n\nNext, we can test the clustering model with the dataset we downloaded in section 1 and preprocessed in the first notebook. We evaluate the model using [V-measure](!https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html), which is the harmonic mean between [homogeneity](!https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score) and [completedness](!https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score). The higher the number, the better the model is, values are between 0 and 1.\n\nAdditionally, we create a baseline model that predicts random clusters for each input. We want our clustering model defined in section 2 to do better than this baseline."}, {"metadata": {}, "cell_type": "code", "source": "def run_model_testing(list_of_groups, df, test_size=50):\n    average_homogeneity_score, average_completeness, average_v_measure = 0, 0, 0\n    avg_baseline_score = 0\n\n    for group in list_of_groups[:test_size]:\n        X_test = list(df[df.label_id.isin(list(group))].Sentence.values)\n        y_test = list(df[df.label_id.isin(list(group))].label_id.values)\n        n = len(df[df.label_id.isin(list(group))].label_id.unique())\n        \n        clusters, cluster_terms = run_model(X_test)\n        \n        average_homogeneity_score += homogeneity_score(y_test, clusters)\n        average_completeness += completeness_score(y_test, clusters)\n        average_v_measure += v_measure_score(y_test, clusters)\n        \n        # baseline\n        baseline_predictions = np.random.choice(np.arange(1, 5), len(X_test))\n        avg_baseline_score += v_measure_score(y_test, baseline_predictions)\n\n    \n    print('Test V Measure:     ', average_v_measure/ test_size)\n    print('Baseline V Measure: ', avg_baseline_score / test_size)\n", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We see that our model actually does do much better than randomly picking clusters. Since the Test V Measure is higher than the Baseline V Measure."}, {"metadata": {}, "cell_type": "code", "source": "run_model_testing(list_of_groups, df)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:8: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n", "name": "stderr"}, {"output_type": "stream", "text": "Test V Measure:      0.5087237379338818\nBaseline V Measure:  0.15418958938016988\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 4. Example  <a class=\"anchor\" id=\"4\"></a>\n\nFinally, we use a sample of comments a retail company could see and run the model on it. \nTo test out your own comments:\n1. Define your comments e.g. `comments = ['comment1', 'comment2', ...]`,\n2. Use `run_model(comments)`, which will return `best_labels` and `top_terms`. e.g. `best_labels, top_terms = run_model(comments)`\n3. Follow the example below to use `print_clustering_result` to print out the groups in a more human readable way\n"}, {"metadata": {}, "cell_type": "code", "source": "comments_5 = [\n    'Customer service was polite.',\n    'The socks are a pretty color but expensive.',\n    'The shirt I bought was green and service was great.',\n    'I think the sweater and socks were perfect.',\n    'I do not like the shoes, so ugly and expensive.',\n]\n\ncomments_20 = [\n    'Out of all the products I bought, the shirt was my favorite because it is comfortable. However, the sweater and socks really missed the mark and were not worth it.',\n    'My order arrived several days late. But when I contacted customer service they were very helpful and refunded me.',\n    'Horrible customer service, I have never met such rude people. Would not recommend at all.',\n    'Everything I ordered arrived perfectly on time and looked exactly like in the pictures! This company has high quality products.',\n    'The company is okay.',\n    'Prices are ridiculous',\n    'Way too overpriced.',\n    'Can never find anything that fits right',\n    'Nice clothes for your teenager.',\n    'They were really well organized and made the experience way less stressful than i thought it would be',\n    'Friendly staff, good range of clothes.',\n    'Fashionable place',\n    'Decent quality product! Friendly customer service',\n    'I really love all the clothes, beautiful. Just a little too expensive',\n    'Great quality of the items, cashier and stocker were very friendly.',\n    'Ugly and overpriced.'\n    'This place was okay and I did find a couple plain shirts for cheap. Overall disappointed with their selection of basics and prices.',\n    'Rude employees. Horrible customer service and limited clothing. Only good thing is cheap clothing',\n    'Service was good and I got a lot for my money',\n    'My favorite brand',\n]", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The following method helps to print out clustering results in a more interpretable way."}, {"metadata": {}, "cell_type": "code", "source": "def print_clustering_result(sentences, labels, top_terms):\n    label_to_sentences = defaultdict(list)\n    for i in range(len(labels)):\n        label_to_sentences[labels[i]].append(sentences[i])\n    \n    number_of_groups = len(label_to_sentences.keys())\n    for i in range(number_of_groups):\n        print('---------------------------------------')\n        print('Group {}. Top Terms: {}'.format(i, top_terms[i]))\n        for j in range(len(label_to_sentences[i])):\n            print('- ' + label_to_sentences[i][j])", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Using the example comments, we can see how the model descided to cluster the sentences."}, {"metadata": {}, "cell_type": "code", "source": "# Automatically determine number of clusters.\n# However can also add the parameter e.g. number_of_clusters=5\nfor comments in [comments_5, comments_20]:\n    best_labels, top_terms = run_model(comments)\n    print_clustering_result(comments, best_labels, top_terms)\n    print('\\n\\n')", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "---------------------------------------\nGroup 0. Top Terms: ['expensive', 'socks', 'pretty color expensive', 'color', 'color expensive']\n- The socks are a pretty color but expensive.\n- I think the sweater and socks were perfect.\n- I do not like the shoes, so ugly and expensive.\n---------------------------------------\nGroup 1. Top Terms: ['service', 'polite', 'service polite', 'customer', 'customer service']\n- Customer service was polite.\n- The shirt I bought was green and service was great.\n\n\n\n---------------------------------------\nGroup 0. Top Terms: ['favorite', 'company', 'really', 'products', 'way']\n- Out of all the products I bought, the shirt was my favorite because it is comfortable. However, the sweater and socks really missed the mark and were not worth it.\n- Everything I ordered arrived perfectly on time and looked exactly like in the pictures! This company has high quality products.\n- The company is okay.\n- They were really well organized and made the experience way less stressful than i thought it would be\n- My favorite brand\n---------------------------------------\nGroup 1. Top Terms: ['clothes', 'good', 'friendly', 'really', 'quality']\n- Nice clothes for your teenager.\n- Friendly staff, good range of clothes.\n- I really love all the clothes, beautiful. Just a little too expensive\n- Great quality of the items, cashier and stocker were very friendly.\n- Service was good and I got a lot for my money\n---------------------------------------\nGroup 2. Top Terms: ['prices', 'place', 'overpriced', 'way', 'okay']\n- Prices are ridiculous\n- Way too overpriced.\n- Can never find anything that fits right\n- Fashionable place\n- Ugly and overpriced.This place was okay and I did find a couple plain shirts for cheap. Overall disappointed with their selection of basics and prices.\n---------------------------------------\nGroup 3. Top Terms: ['customer', 'customer service', 'service', 'horrible customer', 'horrible customer service']\n- My order arrived several days late. But when I contacted customer service they were very helpful and refunded me.\n- Horrible customer service, I have never met such rude people. Would not recommend at all.\n- Decent quality product! Friendly customer service\n- Rude employees. Horrible customer service and limited clothing. Only good thing is cheap clothing\n\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\nThis notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n\nCopyright \u00a9 2020 IBM. This notebook and its source code are released under the terms of the MIT License."}, {"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}