{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Part 1 - Data Exploration & Visualization\n# Load and Visualize IBM Debater\u00ae Thematic Clustering of Sentences\nThis notebook relates to the IBM Debater\u00ae Thematic Clustering of Sentences dataset. The dataset contains 692 articles from Wikipedia, where the number of sections(clusters) in each article ranges from 5 to 12, and the number of sentences per article ranges from 17 to 1614.\n\nThis dataset can be obtained for free from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/thematic-clustering-of-sentences/).\n\nIn this notebook, we load, explore, clean and visualize the dataset.\n\nText Clustering can be applied to texts at different levels, from single words to full documents, and can vary with respect to the clustering goal. In thematic clustering, the aim is to cluster texts based on thematic similarity between them, namely grouping together texts that discuss the same theme. In this dataset \u201cThematic Clustering of Sentences\u201d sentences are annotated for their thematic clusters.\n\n### Table of Contents\n\n* [0. Prerequisite](#prerequisite)\n* [1. Load Data](#1)   \n    * [1.1 About](#abstract)\n    * [1.2 Download and Extract](#download)\n    * [1.3 Preprocessing Data](#preprocess)\n* [2. Data Visualization](#2)\n* [3. Save the Cleaned Data](#3)\n* [Authors](#authors)\n\n\n<a class=\"anchor\" id=\"prerequisite\"></a>\n### 0. Prerequisites\n\nBefore you run this notebook complete the following steps:\n- Insert a project token\n- Import required modules\n\n#### Insert a project token\n\nWhen you import this project from the Watson Studio Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n\n```python\n# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\npc = project.project_context\n```\n\nIf you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n\n* Click on `More -> Insert project token` in the top-right menu section\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n* This should insert a cell at the top of this notebook similar to the example given above.\n\n  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n\n* Run the newly inserted cell before proceeding with the notebook execution below\n\n#### Import required modules\n\nImport and configure the required modules."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# Define required imports\nimport pandas as pd\nfrom pandas import read_excel\nimport numpy as np\nimport matplotlib.pyplot as plt\n# !pip install plotly==4.8.2 \nimport plotly.graph_objs as go\nimport seaborn as sns\n!pip install cufflinks\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom IPython.display import clear_output\nclear_output()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1. Load Data <a class=\"anchor\" id=\"1\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.1 About <a class=\"anchor\" id=\"abstract\"></a>\nThe goal of these notebooks is to use the [IBM's Debater - Thematic Clustering of Sentences](https://developer.ibm.com/exchanges/data/all/thematic-clustering-of-sentences/) dataset to group sentences by their main topics and themes. This could be used in for example an application that collects comments and feedback from customers of a company to help organize the comments.\n\nThis first notebook will focus on exploring the dataset modify it to be used in evaluate our model in the second notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.2 Download and Extract <a class=\"anchor\" id=\"download\"></a>\n\nThis notebook just requires one dataset which is from [IBM's Debater - Thematic Clustering of Sentences](https://developer.ibm.com/exchanges/data/all/thematic-clustering-of-sentences/) named `dataset.csv`. We use the below method to load and read this dataset that is in the Watson Studio Project."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# Define get data file function\ndef get_file_handle(fname):\n    # Project data path for the raw data file\n    data_path = project.get_file(fname)\n    data_path.seek(0)\n    return data_path"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### dataset.csv: \nThis file contains 692 articles from Wikipedia, where the number of sections(clusters) in each article ranges from 5 to 12, and the number of sentences per article ranges from 17 to 1614."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UNIGRAM</th>\n      <th>SENTIMENT_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aa</td>\n      <td>0.019674</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aaa</td>\n      <td>0.032775</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aaas</td>\n      <td>0.074593</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aachen</td>\n      <td>0.011926</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aah</td>\n      <td>0.118070</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "  UNIGRAM  SENTIMENT_SCORE\n0      aa         0.019674\n1     aaa         0.032775\n2    aaas         0.074593\n3  aachen         0.011926\n4     aah         0.118070"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# define filename\nDATA_PATH = 'dataset.csv'\n\n# Using pandas to read the data \n# Since the `DATE` column consists date-time information, we use Pandas parse_dates keyword for easier data processing\ndata_path = get_file_handle(DATA_PATH)\nclustering_df = pd.read_csv(data_path, sep=\" \")\nclustering_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.3 Preprocessing Data <a class=\"anchor\" id=\"preprocess\"></a>\n\nIn order for this data to be used to evaluate a clustering model, we need to assign them clusters. According to the readme file of the dataset, each cluster is each sectionTitle. Thus, you can combine the \"Article Title\" and \"SectionTitle\" to get a group. \n\nWe will add columns in the dataset to more easily show the cluster. `label` is the unique string while `label_id` is a unique number."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "clustering_df['label'] = clustering_df.apply(lambda row: row['Article Title'].replace(\" \", \"_\") + \":\" + row['SectionTitle'].replace(\" \", \"_\"), axis=1)\nclustering_df['label_id'] = clustering_df.label.astype('category').cat.codes\nclustering_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can create a dictionary mapping the label ID to the label name."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "id_to_category = dict( enumerate(clustering_df.label.astype('category').cat.categories) )"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If we take a look at the number of sentences that correspond to each cluster (label), we see that one cluster has a lot more sentences."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# One group has a lot more sentences. \nclustering_df.label_id.value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "id_to_category[32]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We remove this cluster from our dataset because when we test, we want to keep groups together. Having this one very large group may not be an accurate representation of the real data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Remove rows in that top category\ntop_id = clustering_df.label_id.value_counts().index[0] \ndf = clustering_df.loc[(clustering_df.label != id_to_category[top_id])]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Next, we set the features to be `Sentence` which is all the text data we are interested in. We are predicting the `label_id`. Below we see that there are 5554 (1 removed) clusters and on average 8 sentences are in each cluster."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X = df.Sentence\ny = df.label_id\n\nprint('Total data rows: ', len(X))\nprint('Unique groups: ', len(y.unique()))\nprint('Avgerage number of rows per group: ', clustering_df.label_id.value_counts().mean())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To test a model, we will break this dataset into smaller datasets because in the real world, we likely would not want to have 5000 unique clusters. So we will split the data so that each set has about 5 clusters. To do this, we will randomly take 5000 of the 5554 clusters, then split this into 1000 sets. Now we have 1000 sets to test on (`list_of_groups`)."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "np.random.seed(42)  # get reproducible results\nnumber_of_groups = 1000\nsampled_categories = np.random.choice(y.unique(), size=5000)\nlist_of_groups = np.split(sampled_categories, number_of_groups)  # 5 categories in each group"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2. Data Visualization <a class=\"anchor\" id=\"2\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 3. Save the Cleaned Data <a class=\"anchor\" id=\"3\"></a>\n\nFinally, we save the cleaned dataset as a Project asset for later re-use. You should see an output like the one below if successful:\n\n```\n{'file_name': 'bigrams.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'ibmdebatersentimentcompositionlex-donotdelete-pr-jhjwrb2ah5iwb0',\n 'asset_id': '644d1e6c-757e-401c-9ff8-f6090e5ac998'}\n```\n\n**Note**: In order for this step to work, your project token (see the first cell of this notebook) must have `Editor` role. By default this will overwrite any existing file."
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'file_name': 'bigrams.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'ibmdebatersentimentcompositionlex-donotdelete-pr-jhjwrb2ah5iwb0',\n 'asset_id': '644d1e6c-757e-401c-9ff8-f6090e5ac998'}"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "project.save_data(\"unigrams.csv\", unigrams.to_csv(float_format='%g'), overwrite=True)\nproject.save_data(\"bigrams.csv\", bigrams.to_csv(float_format='%g'), overwrite=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n#### Next steps\n\n- Close this notebook.\n- Open the `Part 2 - Model Development` notebook to explore the cleaned dataset.\n\n\n\n<a id=\"authors\"></a> \n### Authors\nThis notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n\nCopyright \u00a9 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}